---
title: "Stat 154 - Quiz 3"
author: "Bret Hart"
date: "October 17, 2016"
output: pdf_document
---

###Question 1

First, we randomly sample a test set from the housing data to set aside and use as a means of predicting the test error, through the validation set approach.

This is done in the R script.

```{r}
#set.seed(82)
#sample.set <- sample(506,455)
#housing.train <- housing[sample.set,]
#housing.test <- housing[-sample.set,]
```


###Question 2


Next, we examine the full correlation matrix to carry out exploratory analysis and gain a better sense of each predictor's relationship with MEDV, our dependent variable.

```{r include = FALSE, cache=FALSE}

load('~/stat154-quiz3/data/data.RData')
load('~/stat154-quiz3/data/bssdata.RData')
load('~/stat154-quiz3/data/fwddata.RData')
load('~/stat154-quiz3/data/shrinkdata.RData')
load('~/stat154-quiz3/data/preddata.RData')

require(xtable)
require(glmnet)

cor.table <- xtable(cor.matrix, caption = "Correlation Matrix of MEDV on 12 Predictors")

```


```{r results='asis', echo=FALSE, cache=FALSE}
#need data 
load('~/stat 154/data.RData')
require(xtable)

cor.table <- xtable(cor.matrix, caption = "Correlation Matrix of MEDV on 12 Predictors")
print(cor.table, comment = FALSE, caption.placement = 'top', table.placement = '!h', 
      size = "\\fontsize{8.5pt}{10pt}\\selectfont")
```

If we merely look at which predictors have the highest absolute R^2 value for the MEDV dependent variable, we can go down from abs(R^2) == 1 to 0.    
We see LSTAT has -.74, which is the highest abs(R^2) value.    
RM has the next highest abs(R^2), but is positive, at .70.   

These are the only two predictors with abs(R^2) above .6. PTRATIO, with -.52 sneaks in as the last predictor with abs(R^2) above .5. These first 2 predictors, LSTAT and RM, and secondarily  PTRATIO, seem as though they are the 3 most relevant predictors for predicting MEDV. 

CRIM: -.41, INDUS: -.48, NOX: -.42, TAX: -.46 are the remaining predictors above .4 - while they may not be the greatest at predicting MEDV, they are still pretty good for a real-world data set. The rest of the predictors likely do not do a great job of predicting MEDV.


###Question 3

Now we implement an algorithm to actually choose a certain combination of predictors to use in our model to predict MEDV.

We carry out a best subset selection. This entails testing every single combination of predictors for each number of possible predictors. While computationally intensive, with only 12 predictors, it's only testing around 4000 models, which takes around 5-10 seconds on my computer. My algorithm can be seen in my .R file, but here is the matrix of each best combination of predictors for each level, with attached R2 value. I used the R2 value to choose the best combination of predictors at every level.

```{r include = FALSE, cache=FALSE}

aa <- 2:13
ee <- 1
for(aa in 2:13){
  interim.models[aa,ee] <- round(as.numeric(interim.models[aa,ee], digits = 8), 4)
  ee <- ee +1
}

#bss.mod.table <- xtable(interim.models, 
#                    caption = "Matrix of each best combination of predictors per level")

```



```{r}

print(interim.models) 
      
```

And here are the first few terms of x, the list which holds the lm object for each best combination of predictors at each level of BSS. Also, here are the first few terms of y, which holds the summary information for each lm object in x.

```{r}

list(x[[1]],x[[2]],x[[3]])
list(y[[1]],y[[2]],y[[3]])

```

Now that we have all best lm objects in x, summaries in y, and the matrix of each predictor combination/R2 value, we can compare the predictor lm combinations by using Cp, BIC, Adjusted R Squared, and Cross Validation to compare more accurately across predictor levels where R2 and RSE cannot.

First, here is our set of Cp statistics at each level.

```{r}

cp.vect

```

And we find the minimum Cp, which belongs to the 10 predictor lm object.

```{r}

cp.min

```

Now, check BIC, and find the minimum BIC across the lm objects. We find the BIC actually suggests the 6 predictor model, not the 10 predictor model.

```{r}

bic.vect
bic.min

```

Now, Adjusted R2, and its minimum. We find that the adj r2 also suggests the 10 predictor model.

```{r}

adj.rsq
adj.max

```

Lastly, we check cross validation. The algorithm for this can be found in my source R code. Here we will just look at the matrix of outputs and their minimum. Again, we find that the 10 predictor model is probably the best. 

```{r}

cv.vect
cv.min

```


Despite the BIC implying otherwise, 3 of 4 statistics which adjust training error and simulate test error suggest that the 10 predictor model is best. Thus, we can pretty confidently assume that this is the best model under BSS. Let's look at the 10 predictor model.

```{r}

x[[10]]
y[[10]]

```

Now that we have a good sense of what the 10 predictor model found by BSS looks and behaves like, and are relatively confident that it is the best of the 4000 or so models tested comprehensively, we can try other methods of regression and model selection to compare.



###Question 4

Although we are doing forward selection next, it is entirely nested within BSS. Forward selection cannot find a 'better' model, it simply runs far less computations than BSS. However, we can compare the two to see if, in this case, forward selection will actually return a similar result at a fraction of the computing power!

We thus carry out forward selection. This entails finding the best predictor at a level, and then keeping that predictor in the model and subsequently testing every other predictor with it, ascertaining the best combination, and continuing from there. It's obviously far less complex, run-time wise, than BSS, but it has no guarantee of getting the same predictor set, as it is necessarily making compromises by keeping a predictor in the model at each level. The algorithm can be found in my .R file, but here is the matrix of each best added predictor model at each level, along with the respective R2 value. I used the R2 value to choose each best subsequent combination of predictors at every level.


```{r include = FALSE, cache=FALSE}

aaa <- 2:13
eee <- 1
for(aaa in 2:13){
  interim.models2[aaa,eee] <- round(as.numeric(interim.models[aaa,eee], digits = 8), 4)
  eee <- eee +1
}

#bss.mod.table <- xtable(interim.models, 
#                    caption = "Matrix of each best combination of predictors per level")

```



```{r}

print(interim.models2) 
      
```



And here are the first few terms of xx, the list which holds the lm object for each best subsequently added combination of predictors at each level of forward selection. Also, here are the first few terms of yy, which holds the summary information for each lm object in xx.

```{r}

list(xx[[1]],xx[[2]],xx[[3]])
list(yy[[1]],yy[[2]],yy[[3]])

```

At least for the first 3 lm objects in xx, they match exactly with the BSS lm objects! 

Now that we have all best lm objects in xx, summaries in yy, and the matrix of each subsequent predictor combination/R2 value, we can compare the predictor lm combinations by using Cp, BIC, Adjusted R Squared, and Cross Validation to compare more accurately across predictor levels where R2 and RSE cannot.

First, here is our set of Cp statistics at each level.

```{r}

cp.vect.for

```

And we find the minimum Cp, which belongs to the 10 predictor lm object, just like the BSS Cp statistic! However, we haven't yet checked to see if the model has the same predictors. We'll do this after calculating the rest of the statistics.

```{r}

cp.min.for

```

Now, check BIC, and find the minimum BIC across the lm objects. We find the BIC actually suggests the 6 predictor model, just like in BSS. Again, we don't know if these models are exactly the same, but it's still interesting that the BIC suggests the same model again. Perhaps this is due to its stricter penalty than the other statistics on number of predictors.


```{r}

bic.vect.for
bic.min.for

```

Now, Adjusted R2, and its minimum. We find that the adj r2 also suggests the 10 predictor model, just as in BSS.

```{r}

adj.rsq.for
adj.max.for

```

Lastly, we check cross validation. The algorithm for this can be found in my source R code. Here we will just look at the matrix of outputs and their minimum. Again, just as before, we find that the 10 predictor model is probably the best. 

```{r}

cv.vect.for
cv.min.for

```


We arrive at the exact same results as in BSS! 3 of 4 statistics, with the BIC saying otherwise, suggest that the 10 predictor model is best, which gives us a pretty strong reason to believe that it is probably the best. Let's look at the two models and see if they're the same. 


```{r}

x[[10]]
xx[[10]]

```

Although it is a bit hard to parse, we can see that the models are actually equivalent in BSS and forward selection! And, while the predictors are the same, they were added in a different order! 

Of course, the conclusion here is still the same. Forward selection gives us the same 10 predictor model as best subset selection, and if it is the literal best subset possible, it's pretty cool that we were able to get it with forward selection, a method of significantly less computational runtime!


###Question 5

Both BSS and forward selection only examine models of pure multiple linear regression. Sure, they can examine a large number of multiple linear regression models and isolate the best ones, but they are testing and showcasing only one possible modeling type. We can try out other modes of regression, such as shrinkage methods like Ridge Regression and the Lasso to see what kinds of models can be produced by adjusting and shrinking coefficients in the multiple linear regression model including/testing all predictors.

We use the package glmnet for this, and thus, do not have to create the algorithm to undergo this regression process. We can examine exactly the functions that create these regressions.

First, let's carry out a ridge regression. We will load these objects from our .R file, but we can still look at the code that was used to produce it. The alpha = 0 method tells glmnet to carry out a ridge regression. We have to use a model.matrix to map the x values, as glmnet is very particular with regards to what format the predictors take.

```{r}

#ridge.reg <- glmnet(model.matrix(MEDV ~ ., housing.train2), MEDV, alpha = 0)

```

And, glmnet includes a function to carry out cv over all possible shrinkage penalties, and can allow us to examine the different lambdas and their associated MSE. Using this cv function, we can plot all of the lambdas tested and their MSEs, and even extract directly the 'best' lambda with best proportional MSE.

```{r}

#ridge.cv <-  cv.glmnet(model.matrix(MEDV ~ ., housing.train2), MEDV, alpha = 0)
plot(ridge.cv)
ridge.cv$lambda.min

```

We find the minimizing and best lambda for the ridge regression is .7445355.

We can undergo a similar procedure for the lasso, with the only adjustment being simply changing the 'alpha = ' method from 0 to 1. Thus, the only change is alpha = 1. Glmnet sure is useful! Again, we will extract the lasso objects from our .R source, but we can still look at the code as it is very simple. 

```{r}

#lasso.reg <- glmnet(model.matrix(MEDV ~ ., housing.train2), MEDV, alpha = 1)
#lasso.cv <-  cv.glmnet(model.matrix(MEDV ~ ., housing.train2), MEDV, alpha = 1)
plot(lasso.cv)
lasso.cv$lambda.min

```

Although the cv lambda vs MSE plot is a bit hard to interpret in this case, we can still extract the best lambda shrinkage penalty directly from the lasso.cv object with the best proportional MSE. In this case, the minimizing and best lambda for the lasso regression is  0.01604053.

Thus, with these two shrinkage methods, we arrive at two additional regression models for the data which we could not have arrived at with simply multiple linear regression, as these methods actually adjust predictors to their calculated, proportional weights. 



###Question 6

Best subset and Forward selection both gave us the 10 predictor model as a favorite, with the BIC the sole believer in a 6 predictor model both times. Both the ridge and lasso regressions are quite different from this 10 predictor model, and we have little reason to think that the 6 predictor multiple linear regression model is very good. So, the best 3 we have are the 10 predictor and the ridge and lasso models. Let's try them and see how they do against the test data we set aside at the beginning!

First, let's look quickly at the 3 regression objects we ended up with.

```{r}

x[[10]]


#x.matrix <- model.matrix(MEDV ~ ., housing.train2)
#ridge.lamb <- glmnet(x.matrix,MEDV, alpha = 0, lambda = ridge.cv$lambda.min)

ridge.lamb$beta

#lasso.lamb <- glmnet(x.matrix,MEDV, alpha = 1, lambda = lasso.cv$lambda.min)
lasso.lamb$beta

```

These aren't the greatest ways of viewing the differing regression objects, but we can examine how good they are better by testing their predictive capability against the set aside testing data. Let's look at the ridge and lasso regressions first.

With the ridge regression, we do a little bit of input manipulation, then end up with the a vector of the predicted values from the observations in the test data, from the ridge regression. We use the glmnet function predict() to do this. Thus, we end up with a vector of predicted values for MEDV, for the test data, derived and predicted from our ridge regression with minimized lambda.

```{r}

#test.matrix <- model.matrix(housing.test$MEDV ~., housing.test)
#ridge.pred <- predict(ridge.reg, s = ridge.cv$lambda.min, newx = test.matrix[,-2])

head(ridge.pred)

```

It should be the same length as the test MEDV vector, and it is! Thus, we subsetted our data correctly and can use the test data set aside to simulate a test error!

```{r}

length(ridge.pred)==length(housing.test$MEDV)

```

Here are the first few residuals (i.e, the predicted values from the ridge regression - the actual MEDV values from the test data). And, from this, we can find the MSE of the ridge regression prediction, that is, the error in prediction from predicted vs actual values.


```{r}

head(ridge.pred - housing.test$MEDV)
mean((ridge.pred - housing.test$MEDV)^2)

```

Thus, our MSE for the ridge regression prediction is 24.45394. We want the lowest one possible. Let's see how the other models do and compare their MSEs.


The lasso regression follows very similarly from the ridge regression, with the only difference in derivation being the alpha and lambda values in predict(). We create a vector of predicted values from the test observations from the lasso regression, examine the residuals of predicted - actual, and calculate the MSE of the lasso regression.

```{r}

#lasso.pred <- predict(lasso.reg, s = lasso.cv$lambda.min, newx = test.matrix[,-2])
head(lasso.pred)
head(lasso.pred - housing.test$MEDV)
mean((lasso.pred - housing.test$MEDV)^2)

```

Our MSE for the lasso regression prediction is 24.17735. This is very marginally lower than the ridge regression, so perhaps the lasso is doing a slightly better job of predicting and modeling the data - but, of course, this subtle of a difference could be due entirely to the specific test data which we used. Before we make any conclusions, let's look at the 10 predictor model from the BSS.

The predict() function for a straightforward multiple linear regression is a little simpler, so we can observe it directly. We'll still load it from our .R source, but the inputs are simply the lm object and the data it's testing/predicting on.

```{r}

#bss.pred <- predict(x[[10]], newdata = housing.test)

```

Just as with the lasso and ridge predictions, we can look at some of the predicted values, look at some of the residuals, and calculate the MSE, the error of prediction from predicted vs. actual.

```{r}

length(bss.pred) == length(housing.test$MEDV)
head(bss.pred - housing.test$MEDV)
mean((bss.pred - housing.test$MEDV)^2)

```

We end up with the lowest MSE, of 23.77013! 

For kicks, let's try out the 6 predictor model that the BIC wants us to try to see if we're really missing something crucial.

```{r}

#bss.pred6 <- predict(x[[6]], newdata = housing.test)
length(bss.pred6) == length(housing.test$MEDV)
head(bss.pred6 - housing.test$MEDV)
mean((bss.pred6 - housing.test$MEDV)^2)

```

As we suspected, however, this 6 predictor model isn't nearly as good as our other models. The MSE of 28.63179 is a good deal worse than the other models. Sorry, BIC!


###Some thoughts/conclusions/the paper


Thus, it seems that when it comes to this specific test data set, straightforward multiple linear regression gives us the best model, that of a specific combination of 10 of the 12 predictors. Of course, we could've tested an infinite number of lambdas for our ridge and lasso regressions to get a slightly better result, but this could perhaps overfit or simply go on for an indeterminate amount of time. It's hard to say why BSS did the best in this case, as it's always complicated to assess the role of so many predictors and such complex math behind each of the regression formats, but it's rewarding as a programmer because the BSS was much harder to implement and program!

It's important to keep in mind that these 3 models were all fitted from a specific subset of our training data and tested on a set of arbitrary test data from our training data. Who knows what the predictive capability or MSE of the different regressions would be when put against different test data sets, or other real-world data points. We have created through this process 3 very similiar in quality regression models, which, perhaps could be viewed simultaneously to predict and assess possible future data points or methods of action. We don't necessarily have to use just 1! Also, realistically, we would use a package for BSS and forward selection to make these algorithms a bit more feasible on the programming end, making it easier to test and examine many different kinds of models and consider their respective pros and cons.

Some interesting things to note are the large coefficient of NOX in all 3 models despite its predicted low coefficient; the fact that the lasso drops the INDUS variable; each predictor in the 10 predictor model has a significant t-value; the 2 variables dropped from the 10 predictor model, (AGE: relative age of houses and INDUS: how residential a neighborhood is) seem to play an intuitive role in determining housing prices, yet, they apparently are not significant enough to truly predict prices; and the relative similarity of each model's MSE despite their vastly different coefficient estimates for each predictor. Data is complicated and there is no such thing as a "best" model. It's important to consider many different ones and view them in juxtaposition and conjunction with each other to ascertain the best sense of the data possible. 


When viewing my findings vs. the findings of the paper, a few things come to mind: they had an additional variable, INC, which seems to have very high correlation with MEDV - higher correlation than any of the predictors which we used. This possibly could make their data set better suited to predicting, as they had fewer confounding (i.e., in multiple linear regression, missing/unnacounted for) variables. They don't talk about it too much explicitly, but it seems like it informed a good deal of their research. The formula they used is a seemingly manually derived adjustment to the least squares regression line upon certain predictors, with weights that I believe they derived through some complex calculations, but are each implemented manually. They didn't just include the coefficients directly: they used the log of DIS, RAD, and STAT, squared RM, and included a few predictors I think they created themselves, as some sort of combination of the given predictors. Their R2 of the finalized model is .82, vs the R2 of our 10-predictor model, .7332. :( This is a significanlty better R2! But, they definitely put a huge amount of work into this project, so I hope they could do something a bit better than just the most basic model selection algorithms. Naturally, this is an intimidating regression model to compare my models to! 

They also discuss at decent length the role they believe each predictor plays: some offer very interesting insight into what makes a house worth what it sells for on the market. Some intuitive predictors are CRIM - crime rate, INDUS - how much industry vs. residential living there is in a neighborhood, and NOX - how toxic the air is. All of these seem to have a reasonable role in determining housing costs. There are some variables which make sense, but seem to practically be strokes of genius to include in this model, in my mind, these are very creative inclusions: CHAS - whether the area is near the local river or not, DIS - a weighted combination of distances to 5 employment centers, RAD - how easy it is to get to nearby highways, and PTRATIO - pupil vs teacher ratio. These seem to have an intuitive role in determining housing prices, but thinking of them, calculating them, and including them in this model really lends some creativity and genius to this prediction. This, to me, is inspiring of the role statisticians can play in the real world - I'm sure they worked closely with community and public policy experts, politicians, etc. to gain a sense of what variables may be less obvious but play a profound role in predicting housing costs. Of course, I find it hard to believe that any of these will really play a more significant role than the LSTAT - the percent of citizens of lower status in the population, but not including them in the model would be irresponsible and short-sighted. We, as statisticians, cannot forget that there can be creativity in all steps of the statistical modeling process, and that we can branch out, interdisciplinarily, to gain a better intution towards what predictors there may be or what our model can really achieve. There is a lot to think about here. 





