Hi, welcome to my machine learning "quiz 3" repository. I examined the housing data set described in https://www.law.berkeley.edu/files/Hedonic.PDF . I'm not sure if the data set I'm using is exactly the same, and the predictors are certainly not in the adjusted final form that they take in the paper. The .csv I used to carry out this analysis is in the data/ directory, and is linked relatively in the R script code. 

I carried out packageless, self-coded best subset selection and forward selection over the data to determine the 'best' multiple linear regression models at each number of possible predictors in the data set. We then perform Cp, BIC, Adjusted R-Squared, and 5-fold cross validation to examine these models against one another, without as much bias towards dimensionality that the natural R2 and RSE contain. We end up deciding that the 10-predictor model, out of 12 possible predictors, is most likely the best fit for this data when considered from both BSS and Forward Selection.

Then, to provide some contrasting regression methods which are not just straightforward, unadjusted multiple linear regression, we carry out a ridge and lasso regression using the glmnet package. We determine the best of a large number of lambda shrinkage penalties by carrying out cross-validation, using the cv.glmnet() function. Thus, we end up with the 'best' ridge and lasso regression methods to compare with our BSS straightforward multiple linear regression model.

Ultimately, we test these 3 regression models on a small 10-90 testing set which we set aside from the beginning, and compare their respective predictive accuracy. We find that, although all 3 MSEs are quite similar, the 10-predictor Multiple Linear Regression derived from BSS seems to slightly edge out the ridge and lasso regression. I acknowledge that I did not test the possible lambda shrinkage penalties as extensively as I could have - but this could be a means to no end. Overfitting, etc. 

I end with some thoughts, conclusions, observations, and comparisons to the original paper. As expected, my models aren't quite up to snuff with the experts, but I acknowledge that my methods are far less sophisticated. 

The data/ directory contains the original .csv and various intermediary .RData files, which are calculated from the main .R script in the script/ directory, which are then loaded into the .Rmd in the report/ directory to supplement our narrative with in-line code. We render this .Rmd file using knitr into a .md, a .tex, and a .pdf report. The .pdf report is the most 'presentable' form of this report. I encourage you to begin there, and supplement your reading with more complete code located in the .R and .Rmd files. It's not perfectly commented, but the main BSS/Forward selection algorithms should be pretty apparent. I'd say the main draw of this project are those algorithms, so I hope you check them out. They can also be found in generalized form, reusable for any project in the functions/ directory. 

Enjoy your stay! 
